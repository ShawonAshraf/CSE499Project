{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Classifier for differentiating image objects based on confidence score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Our goal for the project is to create a system that can detect rotten fruits. In our first week of work, we have created a classifier that can detect rotten fruits. So far we have trained the model with 100+ images of Mangoes. It can not only detect Rotten Mangoes but it can also detect green and ripe mangoes alike with an accuracy of 80%, which we expect to get better as we train it with more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "For this classifier we are using the Inception V3 model from Google which is supplied with their open source Machine Learning framework Tensorflow. Inception V3 model is created using CNN (Convolutional Neural Network).\n",
    "\n",
    "Then using this model we are training our classifier on our training dataset that we collected from Google Image Search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Detail\n",
    "\n",
    "### First Phase\n",
    "\n",
    "The first phase analyzes the images in the created dataset and creates **Bottlenecks** for each of the images in the dataset. Bottleneck is a layer of the classifier that works just before providing the final output. In other words, Bottlenecks are what classify images based on the training data. \n",
    "\n",
    "We iterated the training procedure over every image for 500 iterations. Calculating the layers behind the bottleneck for each image takes a significant amount of time. Since these lower layers of the network are not being modified their outputs can be cached and reused. The lower layers are responsible for edge and shape detection from the images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Graph\n",
    "\n",
    "![graph](img/graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details on the graph\n",
    "\n",
    "The first phase detects edges from the images. The second layer is resoponsible for shape detection. These are considered as lower level layers. The results from these are cached for later use speed up computation.\n",
    "\n",
    "The third layer , conv_1 to conv_4 takes information from the previous two layers, extracts information from the images, applies CNN and creates the bottleneck for the image. \n",
    "\n",
    "Then the bottleneck is fed into the output layer and the training graph is updated. When non dataset image is given as input to the system, the Classifier matches it's features with the bottlenecks in the graph and matches for results. \n",
    "\n",
    "Our initial training gave us 75% validation score and 85% training accuracy for mango images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Training Data\n",
    "\n",
    "We collected all the data representation from our trained graph using Tensorboard.\n",
    "\n",
    "#### Training accuracy\n",
    "\n",
    "The training data has the following labels.\n",
    "\n",
    "- **Training accuracy** : shows the percentage of the images used in the used training data that were labeled with the correct class. We used 3 classes for our data set : mango_ripe, mango_green, mango_rotten\n",
    "\n",
    "- **Validation accuracy:** The validation accuracy is the precision (percentage of correctly-labelled images) on a randomly-selected group of images from a different set. In other words we can say that the Vaidation Accuracy is our expected outcome that we can test against our training results to get the accuracy.\n",
    "\n",
    "- **Cross Entropy** is a loss function that indicates the deviation of the training accuracy from the validation accuracy. From this we can know how well our training is working. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Accuracy\n",
    " - Blue line : Expected / Validation\n",
    " - Orange line : Availed from training\n",
    " \n",
    " ![accuracy](img/accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy\n",
    "\n",
    " - Blue line : Expected / Validation\n",
    " - Orange line : Availed from training\n",
    "\n",
    "![cross](img/cross_entropy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Layer representation \n",
    "\n",
    " - Blue line : Expected / Validation\n",
    " - Orange line : Availed from training\n",
    " \n",
    " ![final_ops](img/final_ops.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Performance \n",
    "\n",
    "Whether our training is working well is indicated by the validation and training accuracy when we apply an image set or image that is not part of the training dataset. If for that image set / image diffrence between validation and training accuracy is very high we can assume the CNN network is overfitting and the training is not going as expected.\n",
    "\n",
    "For our training the difference is acceptable but not as good as expected since we trained with only 100 images. Adding more images with varied and distinct features to the dataset will improve the network. Our target is to achieve 85% or higher for both validation and training accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images : 6\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import tensorflow as tf\n",
    "\n",
    "images = [\n",
    "    'img/green_mangoes.jpg',\n",
    "    'img/green_mango2.jpg',\n",
    "    'img/overripe_ataulfo_mango.png',\n",
    "    'img/ripe_mango_1.jpg',\n",
    "    'img/ripe_mango_2.jpg',\n",
    "    'img/rotten_mango.jpg'\n",
    "]\n",
    "\n",
    "print('Number of images : {}'.format(len(images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to get output from the graph\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "def classify(image_path):\n",
    "    # Reads in the image_data\n",
    "    image_data = tf.gfile.FastGFile(image_path, 'rb').read()\n",
    "\n",
    "    # Loads label file, strips off carriage return\n",
    "    label_lines = [line.rstrip() for line \n",
    "                   in tf.gfile.GFile(\"../retrained_labels.txt\")]\n",
    "\n",
    "    # Unpersists graph from file\n",
    "    with tf.gfile.FastGFile(\"../retrained_graph.pb\", 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # Feeds the image_data as input to the graph and get first prediction\n",
    "        softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')\n",
    "    \n",
    "        predictions = sess.run(softmax_tensor, \\\n",
    "             {'DecodeJpeg/contents:0': image_data})\n",
    "    \n",
    "        # Sorted to show labels of first prediction in order of confidence\n",
    "        top_k = predictions[0].argsort()[-len(predictions[0]):][::-1]\n",
    "    \n",
    "        print('\\nClassified input image : Results :\\n')\n",
    "        for node_id in top_k:\n",
    "            human_string = label_lines[node_id]\n",
    "            score = predictions[0][node_id]\n",
    "            print('Input is : {} \\twith score = {} %'.format(human_string, score * 100))\n",
    "\n",
    "        print('\\nDONE===========================\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result for green_mangoes.jpg \n",
    "\n",
    "![img](img/green_mangoes.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified input image : Results :\n",
      "\n",
      "Input is : green mango \twith score = 81.32983446121216 %\n",
      "Input is : rotten mango \twith score = 12.711438536643982 %\n",
      "Input is : ripe mango \twith score = 5.958719924092293 %\n",
      "\n",
      "DONE===========================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classify(images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ripe_mango_1.jpg\n",
    "\n",
    "![img](img/ripe_mango_1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified input image : Results :\n",
      "\n",
      "Input is : ripe mango \twith score = 67.17912554740906 %\n",
      "Input is : green mango \twith score = 17.06155836582184 %\n",
      "Input is : rotten mango \twith score = 15.759308636188507 %\n",
      "\n",
      "DONE===========================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classify(images[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rotten_mango.jpg\n",
    "\n",
    "![img](img/rotten_mango.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified input image : Results :\n",
      "\n",
      "Input is : rotten mango \twith score = 67.93732047080994 %\n",
      "Input is : ripe mango \twith score = 31.19925856590271 %\n",
      "Input is : green mango \twith score = 0.8634287863969803 %\n",
      "\n",
      "DONE===========================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classify(images[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "Our work for the first week has managed to produce a prototype that can detect rotten mangoes, ripe mangoes and green mangoes with a few hit and misses. We plan to improve it by feeding more images and getting more accuracy. Once we are able to do this with mangoes we can extend the same procedure for other fruits as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related research work\n",
    "\n",
    "- *Going Deeper with Convolutions* (Szegedy et el. 2015, http://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Materials\n",
    "\n",
    "- Tensorflow Official Documentation\n",
    "- Tensoflow for Poets Codelab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### This document was prepared with Jupyter Notebook with `python3` backend."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
